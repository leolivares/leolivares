{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importamos las librerias que utilizaremos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cargamos los datos de los archivos\n",
    "data_ciu = pd.read_table('ciudades.tsv' , header = None)\n",
    "data_tw = pd.read_table(\"tweets_100k.tsv\", encoding = \"utf-8\" , error_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "data_ciu.columns = [\"Ciudad\" , \"Latitud\" , \"Longitud\" , \"Prob\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Utilizamos la funcion multinomial y where para obtener una lista de ciudades que generaron tweets\n",
    "N = 100000\n",
    "probs= data_ciu[\"Prob\"].tolist() #Se considero sumarle 0.001 mas a la probabilidad de Castro\n",
    "\n",
    "_,m = np.where(np.random.multinomial(1, probs, size=(N))) #lista de ciudades por int\n",
    "\n",
    "ciudades = []\n",
    "for n in m:\n",
    "    ciudades += [(data_ciu[\"Ciudad\"].tolist())[n]] #lista de ciudades por string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Definimos una funcion para obtener la cantidad de tweets por ciudad\n",
    "def generar_pob(x):\n",
    "    porc = []\n",
    "    x = x.tolist()\n",
    "    for i in range(10) :\n",
    "        porc += [x.count(i)]\n",
    "    return porc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Utilizamos la funcion multivariate_normal para generar puntos a partir de una lat y long central\n",
    "latitudes = data_ciu[\"Latitud\"].tolist()\n",
    "longitudes = data_ciu[\"Longitud\"].tolist()\n",
    "\n",
    "cantidades = generar_pob(m) #Llamamos la funcion anterior\n",
    "nuevas_c =[]\n",
    "for i in cantidades :\n",
    "    nuevas_c += [i/1000]\n",
    "nuevas_lat = []\n",
    "nuevas_lon = []\n",
    "for i in m:\n",
    "    cant = nuevas_c[i]\n",
    "    mean = [latitudes[i],longitudes[i]] #Por cada tweet se llama la funcion segun la ciudad origen\n",
    "    cov = [[cant*0.0003,0],[0,cant*0.0003]] #La probabilidad de dispersion de los tweets aumenta segun la cant de tweets\n",
    "    x,y = np.random.multivariate_normal(mean, cov, 1).T\n",
    "    nuevas_lat += x.tolist()\n",
    "    nuevas_lon += y.tolist()\n",
    "#BONUS: Se utilizo la funcion multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Agregamos a cada fila del dataframe data_tw, una ciudad de origen con su respectiva lat y longitud generada\n",
    "data_tw[\"ciudad\"] = ciudades\n",
    "data_tw[\"lat\"] = nuevas_lat\n",
    "data_tw[\"lon\"] = nuevas_lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardamos la tabla (data_tw con modificaciones) en un archivo .csv (tweets_pos)\n",
    "data_tw.to_csv(\"tweets_pos.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utilizamos la funcion uniforme para seleccionar 1000 tweets del dataframe anterior\n",
    "a = 0\n",
    "b = 99999\n",
    "pts = np.random.uniform(a, b, 1000)\n",
    "\n",
    "nueva = []\n",
    "for pt in pts :\n",
    "    nueva += [int(pt)]#Cambiamos a int para obtener las posiciones de los tweets seleccionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardamos los datos obtenidos en listas\n",
    "tw_selecc = []\n",
    "date_selecc = []\n",
    "name_selecc = []\n",
    "lat_selecc = []\n",
    "lon_selecc = []\n",
    "ciudad_selecc = []\n",
    "for num in nueva :\n",
    "    tw_selecc += [data_tw.loc[num][\"text_tweet\"]]\n",
    "    date_selecc += [data_tw.loc[num][\"creation_date\"]]\n",
    "    name_selecc += [data_tw.loc[num][\"screen_name\"]]\n",
    "    ciudad_selecc += [data_tw.loc[num][\"ciudad\"]]\n",
    "    lat_selecc += [data_tw.loc[num][\"lat\"]]\n",
    "    lon_selecc += [data_tw.loc[num][\"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creamos un DataFrame con los datos obtenidos, a partir de un diccionario\n",
    "dicc = {\"screen_name\" : name_selecc,\n",
    "         \"creation_date\" : date_selecc,\n",
    "         \"text_tweet\" : tw_selecc,\n",
    "         \"ciudad\" : ciudad_selecc,\n",
    "         \"lat\" : lat_selecc,\n",
    "         \"lon\" : lon_selecc}\n",
    "\n",
    "\n",
    "sample = pd.DataFrame.from_dict(dicc)\n",
    "sample =  sample[['screen_name','creation_date','text_tweet','ciudad','lat','lon']] #Organizamos las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardamos la tabla (sample) en un archivo .csv (tweets_pos_sample)\n",
    "sample.to_csv(\"tweets_pos_sample.csv\",encoding='utf-8')\n",
    "#10 seg de proceso aprox."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
